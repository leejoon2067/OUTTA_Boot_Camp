{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867d58fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a5333a",
   "metadata": {},
   "source": [
    "### 엑셀 파일 생성하기\n",
    "파일명, 시트명, 컬럼명을 정해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f049e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#엑셀파일 생성\n",
    "wb = openpyxl.Workbook(\"raw_data.xlsx\")\n",
    "ws = wb.create_sheet(\"sheet 1\")\n",
    "ws.append(['브랜드','상품명','카테고리','정가','할인가','아이디','별점','피부정보','피부타입','피부고민','자극도'])  #컬럼명 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c761efa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################제공하는 코드 건드리지 마세요 #############################\n",
    "\n",
    "#크롬 드라이버 자동 업데이트\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# 브라우저 꺼짐 방지\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_experimental_option('detach', True)\n",
    "\n",
    "# 불필요한 에러 메세지 없애기\n",
    "chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "# 크롬 드라이버 경로 지정\n",
    "driver_path = \"C:/Users/joons/Documents/OUTTA-project/chromedriver.exe\"  # 여기에 자신이 설치한 크롬 드라이버의 경로를 입력\n",
    "\n",
    "# 크롬 드라이버 생성\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2202a3e1",
   "metadata": {},
   "source": [
    "### 올리브영 스킨케어 랭킹 링크를 main_url로 두고 5위까지 제품 상세 링크를 sub_url에 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a960ddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_url = \"https://www.oliveyoung.co.kr/store/main/getBestList.do?dispCatNo=900000100100001&fltDispCatNo=10000010001&pageIdx=1&rowsPerPage=8\"\n",
    "\n",
    "response = requests.get(main_url)\n",
    "html = response.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "links = soup.select(\"#Container > div.best-area > div.TabsConts.on\") #빈칸채우기\n",
    "\n",
    "#for반복문을 활용하여 5위까지 제품별 상세링크 sub_url에 저장\n",
    "rank = 5\n",
    "\n",
    "sub_url = []\n",
    "\n",
    "# 제품 상세 링크를 li_list에 저장\n",
    "for link in links:\n",
    "    li_list = link.select(\"ul > li\")\n",
    "\n",
    "    for li in li_list:\n",
    "        aTag = li.select_one(\"div > a\")\n",
    "        href = aTag[\"href\"]\n",
    "        if len(sub_url) >= rank: # rank 만큼의 url을 sub_url 리스트에 저장\n",
    "            break    \n",
    "        sub_url.append(href)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "sub_url\n",
    "# 1위 인덱스 0,  2위 인덱스1, 3위 인덱스2, 4위 인덱스3, 5위 인덱스4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905dc1dd",
   "metadata": {},
   "source": [
    "### 크롤링 함수 만들기\n",
    "제품 상세 페이지에서 상단 제품정보(브랜드명,상품명,카테고리,정가,할인가)와  \n",
    "하단의 리뷰고객 좌픅정보(아이디,별점,피부정보)와 우측정보(피부타입,피부고민,자극도)를 가져옵니다.  \n",
    "위의 데이터를 엑셀에 계속 추가하는 함수를 만듭니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22ebe0b",
   "metadata": {},
   "source": [
    "코드의 효율성을 위해 리뷰 고객의 정보는 리스트 형태로 for문의 바깥에서 저장하고, \n",
    "\n",
    "for문 안에서 리스트의 인덱스에 접근하는 방식을 사용했다.\n",
    "\n",
    "이렇게 하지 않으면 for문을 한번 돌 때마다 리스트를 매번 만들어야 하므로 시간이 오래 걸림."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9644353f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def customer_info(url, ws):\n",
    "\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "    # 리뷰 고객의 정보를 미리 리스트로 저장\n",
    "    # 아이디\n",
    "    _id_css_selector = f'#gdasList > li > div.info > div > p.info_user > a.id'\n",
    "    _id_list = driver.find_elements(By.CSS_SELECTOR, _id_css_selector)\n",
    "\n",
    "    # 별점\n",
    "    _star_css_selector = f'#gdasList > li > div.review_cont > div.score_area > span.review_point > span'\n",
    "    _star_list = driver.find_elements(By.CSS_SELECTOR, _star_css_selector)\n",
    "\n",
    "    # 고객 피부 정보\n",
    "    _info_css_selector = f'#gdasList > li > div.info > div > p.tag'\n",
    "    _info_list = driver.find_elements(By.CSS_SELECTOR, _info_css_selector)\n",
    "\n",
    "    # 피부 타입\n",
    "    skin_type_css_selector = f'#gdasList > li > div.review_cont > div.poll_sample > dl:nth-child(1) > dd > span'\n",
    "    skin_type_list = driver.find_elements(By.CSS_SELECTOR, skin_type_css_selector)\n",
    "\n",
    "    # 피부 고민\n",
    "    skin_trouble_css_selector = f'#gdasList > li > div.review_cont > div.poll_sample > dl:nth-child(2) > dd > span'\n",
    "    skin_trouble_list = driver.find_elements(By.CSS_SELECTOR, skin_trouble_css_selector)\n",
    "\n",
    "    # 자극도\n",
    "    skin_irritation_css_selector = f'#gdasList > li > div.review_cont > div.poll_sample > dl:nth-child(3) > dd > span'\n",
    "    skin_irritation_list = driver.find_elements(By.CSS_SELECTOR, skin_irritation_css_selector)\n",
    "\n",
    "\n",
    "    for j in range(0,10,1):\n",
    "\n",
    "        #브랜드\n",
    "        try:\n",
    "            brand = soup.select(\"#moveBrandShop\")[0].get_text()\n",
    "        except:\n",
    "            brand =\"없음\"\n",
    "\n",
    "        #상품명\n",
    "        try:\n",
    "            p_name = soup.select(\"#Contents > div.prd_detail_box.renew > div.right_area > div > p.prd_name\")[0].get_text()\n",
    "        except:\n",
    "            p_name=\"없음\"\n",
    "\n",
    "        #카테고리\n",
    "        try:\n",
    "            p_category = soup.select(\"#smlCatNm\")[0].get_text()\n",
    "        except:\n",
    "            p_category=\"없음\"\n",
    "\n",
    "        #정가\n",
    "        try:\n",
    "            price = soup.select(\"#Contents > div.prd_detail_box.renew > div.right_area > div > div.price > span.price-1 > strike\")[0].get_text()\n",
    "        except:\n",
    "            price=0\n",
    "\n",
    "        #할인가\n",
    "        try:\n",
    "            discount = soup.select(\"#Contents > div.prd_detail_box.renew > div.right_area > div > div.price > span.price-2 > strong\")[0].get_text()\n",
    "        except:\n",
    "            discount=0\n",
    "\n",
    "\n",
    "        #고객 아이디\n",
    "        try:\n",
    "            _id = _id_list[j].text\n",
    "        except:\n",
    "            _id =\"없음\"\n",
    "\n",
    "        #별점\n",
    "        try:\n",
    "            _star = _star_list[j].text\n",
    "        except:\n",
    "            _star = \"없음\"\n",
    "\n",
    "        #고객 피부 정보\n",
    "        try:\n",
    "            _info = _info_list[j].text\n",
    "        except:\n",
    "            _info = \"없음\"\n",
    "\n",
    "        #피부 타입\n",
    "        try:\n",
    "            skin_type = skin_type_list[j].text\n",
    "        except:\n",
    "            skin_type = \"없음\"\n",
    "            \n",
    "        #피부 고민\n",
    "        try:\n",
    "            skin_trouble = skin_trouble_list[j].text\n",
    "        except:\n",
    "            skin_trouble = \"없음\"\n",
    "\n",
    "        #자극도\n",
    "        try:\n",
    "            skin_irritation = skin_irritation_list[j].text\n",
    "        except:\n",
    "            skin_irritation = \"없음\"\n",
    "\n",
    "        #엑셀 데이터 쌓기\n",
    "        ws.append([brand,p_name,p_category,price,discount,_id, _star, _info, skin_type, skin_trouble, skin_irritation])\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d4d340",
   "metadata": {},
   "source": [
    "### 웹페이지 해당 주소로 이동하여 크롤링함수를 실행하는 반복문을 만듭니다.\n",
    "반복문에 들어가야 하는 것  \n",
    "1. 리뷰버튼 클릭  \n",
    "2. 화면 맨 아래까지 스크롤하기 (코드제공)\n",
    "3. 10페이지 이하일 때 : 10페이지 크롤링 한 후 다음 페이지 화살표 버튼 누르기 , 마지막 페이지인 경우 last_page==True\n",
    "4. 11페이지부터 규칙을 찾아 다음 페이지로 이동하게 만들기, 마지막 페이지인 경우 last_page==True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b2304c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#웹페이지 해당 주소 이동\n",
    "for i in range(0,5):  #전체 제품을 한번에 크롤링하지 않고 나눠서 크롤링 할 경우 sub_url 인덱스 고려해서 range숫자 변경\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.implicitly_wait(5)  #웹페이지 로딩 될때까지 5초는 기다림\n",
    "    driver.maximize_window()   #화면 최대화\n",
    "    driver.get(sub_url[i])     #url 가져오기\n",
    "    time.sleep(3)\n",
    "\n",
    "    #리뷰버튼 클릭\n",
    "    rv = driver.find_element(By.CSS_SELECTOR, \"#reviewInfo\")\n",
    "    rv.click()\n",
    "    time.sleep(3)\n",
    "\n",
    "\n",
    "    #리뷰 하단 끝까지 스크롤하는 함수 (빈칸없음.그대로 코드 사용가능)\n",
    "    before_h = driver.execute_script(\"return window.scrollY\")         #스크롤 전 높이\n",
    "    #화면 맨아래까지 스크롤\n",
    "    while True:\n",
    "        driver.find_element(By.CSS_SELECTOR,\"body\").send_keys(Keys.END)\n",
    "        time.sleep(1)\n",
    "\n",
    "        #스크롤 후 높이\n",
    "        after_h = driver.execute_script(\"return window.scrollY\")\n",
    "\n",
    "        #스크롤 값이 같으면 스크롤 멈춤\n",
    "        if after_h == before_h:\n",
    "            break\n",
    "        before_h = after_h   \n",
    "\n",
    "    last_page=False\n",
    "    \n",
    "    for k in range(1,101):  #100페이지 크롤링 한다고 했을 때 (상품당 최대 100페이지까지 있음)\n",
    "\n",
    "        #마지막 페이지면 종료\n",
    "        if last_page == True:\n",
    "            break\n",
    "\n",
    "        #페이지 숫자 10이하 일 때\n",
    "        if k < 11:\n",
    "            try:    \n",
    "                if k != 10:\n",
    "                    current_page_element = driver.find_element(By.CSS_SELECTOR, '.pageing strong[title=\"현재 페이지\"]')\n",
    "                    current_page_number = int(current_page_element.text)\n",
    "                    customer_info(sub_url[i], ws)  #해당 페이지의 리뷰 크롤링\n",
    "\n",
    "                    next_page_number = current_page_number + 1\n",
    "\n",
    "                    next_page_element = driver.find_element(By.CSS_SELECTOR, f'.pageing a[data-page-no=\"{next_page_number}\"]')\n",
    "                    next_page_element.click()\n",
    "                    time.sleep(3)\n",
    "                    \n",
    "                elif k == 10:          #10페이지 크롤링 한 후에 다음페이지로 가는 화살표 버튼 클릭\n",
    "                    next_page_element = driver.find_element(By.CSS_SELECTOR, '.pageing .next')\n",
    "                    next_page_element.click()\n",
    "                    time.sleep(3)\n",
    "                    \n",
    "            except:\n",
    "                break\n",
    "\n",
    "       #페이지 숫자 11이상 일 때  (규칙을 찾아 각 페이지 크롤링 후 다음 페이지로 이동하도록 코드 작성)        \n",
    "        elif k > 10 :\n",
    "            try:\n",
    "                if k % 10 != 0:\n",
    "                \n",
    "                    current_page_element = driver.find_element(By.CSS_SELECTOR, '.pageing strong[title=\"현재 페이지\"]')\n",
    "                    current_page_number = int(current_page_element.text)\n",
    "                    customer_info(sub_url[i], ws)\n",
    "\n",
    "                    next_page_number = current_page_number + 1\n",
    "\n",
    "                    next_page_element = driver.find_element(By.CSS_SELECTOR, f'.pageing a[data-page-no=\"{next_page_number}\"]')\n",
    "                    next_page_element.click()     \n",
    "                    time.sleep(3)\n",
    "\n",
    "                else:\n",
    "                    try:\n",
    "                        page_elements = driver.find_elements(By.CSS_SELECTOR, '.pageing a[data-page-no]')\n",
    "                        page_numbers = [element.get_attribute('data-page-no') for element in page_elements]\n",
    "\n",
    "                        next_page_element = driver.find_element(By.CSS_SELECTOR, '.pageing .next')\n",
    "                        next_page_element.click()\n",
    "                        time.sleep(3)           \n",
    "                    except:\n",
    "                        break\n",
    "                \n",
    "            except last_page == True:\n",
    "                break\n",
    "\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362fa376",
   "metadata": {},
   "source": [
    "### 크롤링한 결과를 엑셀에 저장 (상단에서 만든 엑셀 파일명과 동일하게)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d616c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb.save(\"raw_data.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
